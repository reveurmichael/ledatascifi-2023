{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many (and Better) Models\n",
    "\n",
    "At this point, we can \n",
    "1. Set up a pipeline that \n",
    "    - preprocesses different variables differently\n",
    "    - _we could add an estimator in the middle to reduce the number of variables, since too many variables can lead to overfitting_\n",
    "    - ends in an estimator\n",
    "1. Set up a cross-validation method, and optimize the pipeline parameters via `GridSearchCV` (and save the best model to use on the test sample later)\n",
    "\n",
    "The [best practices page](04b_best_practices) outlined a pseudo-code that covers a full project. Following that code, what is left, is for us to try many more models in the pursuit of improving our predictions of the interest rate! \n",
    "\n",
    "## Better Models\n",
    "\n",
    "Improving the performance of a model is contingent on the problem domain, data, and the models you're considering, so generic advice is tough to offer with complete confidence. That said, these are usually good ideas:\n",
    "\n",
    "- Exploring your data endlessly \n",
    "- Preprocessing data in a pipeline (data leakage is bad!) that utilizes what you learned via EDA (imputation, scaling, and transformations)\n",
    "- Exploring preprocessing alternatives in your pipeline optimization\n",
    "- Feature engineering: Creating new variables via interactions (think: X3 = X1*X2)\n",
    "- Feature selection/reduction: Which X variables to include. Too many variables will lead to overfitting. \n",
    "    - Common options: `SelectFromModel`, `LassoCV`, `RFECV`\n",
    "- Gradient Boosting, discussed [here](https://www.kaggle.com/kashnitsky/topic-10-gradient-boosting) and [here](https://www.youtube.com/watch?v=yrTW5YTmFjw), and ensemble + stacked predictors\n",
    "    - `xgboost` and `lightGBM` are the go to implementations, and `HistGradientBoostingRegressor` is the analogue in sk-learn \n",
    "    - If you just sk-learn for gradient boosting, look for the \"Hist\" in the function name! (Newer, much faster.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
