{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Significant_ warnings about \"statistical significance\"\n",
    "\n",
    "The \"classical\" approach to assess whether X is related to y is to see if the t-stat on X is above 1.96/the p-value is below 0.05. These thresholds are important and part of a sensible approach to learning from data, but when you read about a \"statistically significant relationship\" on some website, it often comes across like \n",
    "<br>\n",
    "<center>\"LOOK YONDER, AT HOW MORE X <b> CAUSES </b> Y! <br>\n",
    "    MY NEW, AND MEANINGFUL, FINDING <br>\n",
    "    IS TRUE <br> \n",
    "    (TRULY!) (VERILY!)  <br>\n",
    "    AND KNOWING IT ... <br> \n",
    "    WILL CHANGE YOUR LIFE!\"</center>\n",
    "\n",
    "And suddenly, you see a article saying that 10 cups of coffee/2 bars of chocolate/3 glasses of wine/etc a day **leads to** longer lives, or that breastfeeding for up to two years **causes** better outcomes.[^babies]\n",
    "\n",
    "[^babies]: The AAP recently started suggesting breastfeeding for two years, in part due to some studies finding a correlation between long breastfeeding and better maternal outcomes. However, moms that breastfeed that long are different than those who don't. One difference: They tend to be richer. (Please pardon the sassy joke: Perhaps the AAP should suggest bringing your child home in a Mercedes.) Even if the study can control for wealth, it's easy to worry about other confounding factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Correlation is not causation\"\n",
    "\n",
    "Surely, you've heard that. I prefer this version:\n",
    "\n",
    "```{epigraph}\n",
    "Everyone who confuses correlation with causation eventually ends up dead.\n",
    "\n",
    "_@rauscher_emily_\n",
    "```\n",
    "\n",
    "The \"default\" interpretation you should have of a regression is that you're seeing a correlation, not that X causes Y. You need to rule out some alternative possibilities first.\n",
    "\n",
    "## Alternatives to causation\n",
    "\n",
    "Here are some reasons the (statistically significant) correlation might not be causal:\n",
    "\n",
    "- Spurious correlation: If you look at enough Xs and enough ys, you, by chance alone, can find \"significant\" relationships where [none exist](https://www.tylervigen.com/spurious-correlations)\n",
    "- Sampling bias: The famous [Dewey Defeats Truman](https://en.wikipedia.org/wiki/Dewey_Defeats_Truman) headline happened because of bad polls (and an analyst that got 4 out of 5 of the prior elections correct)\n",
    "- Survivorship bias: If you evaluate the trading strategy \"buy and hold **current** S&P companies\" for the last 50 years, you'll discover that this trading strategy did great!\n",
    "- Reusing the data aka \"p-hacking\": If you torture the data, it will confess! [Play this fun game](https://fivethirtyeight.com/features/science-isnt-broken/#part1) and you'll see that (1) The choices you make about what variables to include or focus on can change the sign and p-values. (2) If you play with a dataset long enough, you'll find \"results\". \n",
    "- Sample selection: The sample only exists for some subset of possible X or Y values.\n",
    "- Reverse causation: Y causes X.\n",
    "- Omitted variables: W causes X and Y to go up, but if you run a test using just X and Y (not W) you'll find that X and Y are related. \"Ability\" and \"quality\" can not be measured, but are often important to control for. \n",
    "- Simultaneity: Think of this as \"equilibrium effects\". X and Y are determined together, like price and quantity. \n",
    "\n",
    "If you see a regression or a study where these might come up, it's time to think critically about whether you should trust and act on that finding, or do additional tests to prove the relationship is causal.\n",
    "\n",
    "## Getting to causation\n",
    "\n",
    "Generally, the intuition of approaches to proving causality are about finding or creating randomness in X. If variation in X is truly random, then we can attribute different outcomes Y to the differences in X. \n",
    "\n",
    "The most common methods that _can_ to establish causality are:\n",
    "- Randomized trials\n",
    "- Difference in difference\n",
    "- Instrumental variable\n",
    "\n",
    "I emphasized that these methods _can_ establish causality because they do not always suffice. Designing studies to deal with these issues is a massive topic you can pursue in other classes. I can't do it justice here. \n",
    "\n",
    "```{admonition} Humility is good\n",
    ":class: important\n",
    "\n",
    "Until you learn about the advance techniques above, focus on humility as you report regressions: \n",
    "1. Our standard fill in the blank [interpretation sentence](02d_interpretingCoefs)  calls the relationship an \"association\".\n",
    "2. Emphasize in discussion of findings what you found (a statistical association) and didn't (\"We acknowledge that this finding isn't causal.\" \"One limitation of our study is that...\")\n",
    "3. Discuss alternative explanations (some may apply in your setting, some may not)\n",
    "4. Banned words: impact, causes, causality, because of, leads to, etc.\n",
    "\n",
    "```\n",
    "\n",
    "## Help me help you\n",
    "\n",
    "```{tip}\n",
    "When you run a regression, your focus should be on testing and evaluating a hypothesis, not \"finding a result\"\n",
    "```\n",
    "\n",
    "Remember, if you torture the data enough, it will confess and produce a \"statistical\" result. Meaning: It's often \"easy\" to find results. \n",
    "\n",
    "The focus on p-values can be dangerous because it distorts the incentives of analysts. If you're paid to publish research, and journals have a bias towards publishing non-null results (they do), then your incentive is to \"find something.\" [This 538 article](https://fivethirtyeight.com/features/science-isnt-broken/#part1) mentions that about 2/3 of retractions are due to misconduct.\n",
    "\n",
    "However, it doesn't take ill intent: You, or friends, or strangers might find a false result and trumpet it due to **motivated reasoning, cognitive dissonance, or confirmation bias.** Analysis in many domains are fraught with these temptations; [the game above](https://fivethirtyeight.com/features/science-isnt-broken/#part1) has a political valence. \n",
    "\n",
    "Additionally, the focus on p-values shifts attention towards statistical significance, which does not mean causation nor economic significance (i.e. large/important relationships)**\n",
    "\n",
    "```{admonition} Tips to avoid p-hacking\n",
    ":class: tips\n",
    "1. Your focus should be on testing and evaluating a hypothesis, not \"finding a result\"\n",
    "1. Null results are fine!    Famously, Edison and his teams found a lot of wire filaments that did _not_ work for a lightbulb, and this information was valuable!\n",
    "1. \"Preregister\" your ideas\n",
    "    - The simplest version of this: Write down your data, theory, and hypothesis (it can be short!) **BEFORE** you run your tests.\n",
    "    - [This Science article](https://www.science.org/content/article/more-and-more-scientists-are-preregistering-their-studies-should-you) covers the reasoning and intuition for it\n",
    "    - [This article by Brian Nosek, one of the key voices pushing for ways to improve credibility](https://www.pnas.org/content/115/11/2600) is an instant classic\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
